import scipy.io as sio
mat_contents = sio.loadmat('cu_ecg_small_data_50.mat')  # 예시: XTrain.mat 파일 불러오기

mat_file_train = sio.loadmat('cu_ecg_small_data_50.mat') # Train 데이터 Mat 파일 불러오기
mat_file_test = sio.loadmat('cu_ecg_small_data_50.mat') # Test 데이터 Mat 파일 불러오기

X_train = mat_file_train['XTrain'] # Train 데이터 추출
X_test = mat_file_test['XTest'] # Test 데이터 추출

y_train = mat_file_train['YTrain'] # Train 레이블 추출
y_test = mat_file_test['YTest'] # Test 레이블 추출

from keras.utils import to_categorical

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)


from keras.utils import to_categorical

y_train = to_categorical(y_train)


from keras.utils import to_categorical

y_test = to_categorical(y_test)

print(X_train.shape)

# 데이터 크기 조절
X_train = X_train[:2000, :]
y_train = y_train[:2000]

X_test = X_test[:1000, :]
y_test = y_test[:1000]

from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D

model = Sequential()
model.add(Conv2D(16, kernel_size=3, activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(32, kernel_size=3, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

import numpy as np
from keras.utils import np_utils
from keras.datasets import mnist

# MNIST 데이터 로드
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# 데이터 전처리
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# 정규화
X_train /= 255
X_test /= 255

# one-hot encoding
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))


model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
              
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))


결과 :

Epoch 1/10
1875/1875 [==============================] - 79s 41ms/step - loss: 0.1410 - accuracy: 0.9557 - val_loss: 0.0471 - val_accuracy: 0.9870
Epoch 2/10
1875/1875 [==============================] - 76s 41ms/step - loss: 0.0444 - accuracy: 0.9869 - val_loss: 0.0438 - val_accuracy: 0.9865
Epoch 3/10
1875/1875 [==============================] - 72s 38ms/step - loss: 0.0321 - accuracy: 0.9901 - val_loss: 0.0292 - val_accuracy: 0.9911
Epoch 4/10
1875/1875 [==============================] - 74s 40ms/step - loss: 0.0234 - accuracy: 0.9927 - val_loss: 0.0321 - val_accuracy: 0.9910
Epoch 5/10
1875/1875 [==============================] - 69s 37ms/step - loss: 0.0200 - accuracy: 0.9941 - val_loss: 0.0264 - val_accuracy: 0.9922
Epoch 6/10
1875/1875 [==============================] - 69s 37ms/step - loss: 0.0168 - accuracy: 0.9952 - val_loss: 0.0338 - val_accuracy: 0.9908
Epoch 7/10
1875/1875 [==============================] - 71s 38ms/step - loss: 0.0137 - accuracy: 0.9962 - val_loss: 0.0346 - val_accuracy: 0.9912
Epoch 8/10
1875/1875 [==============================] - 71s 38ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0369 - val_accuracy: 0.9922
Epoch 9/10
1875/1875 [==============================] - 69s 37ms/step - loss: 0.0106 - accuracy: 0.9973 - val_loss: 0.0387 - val_accuracy: 0.9921
Epoch 10/10
1875/1875 [==============================] - 69s 37ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.0498 - val_accuracy: 0.9903



해석:



위 결과는 총 10개의 epoch으로 학습을 진행한 결과입니다. 각 epoch마다 train 데이터셋으로 학습을 하고, validation 데이터셋으로 모델의 성능을 평가합니다.

첫 번째 epoch에서 train 데이터셋의 loss는 0.1410, accuracy는 0.9557이며, validation 데이터셋의 loss는 0.0471, accuracy는 0.9870입니다. 즉, 첫 번째 epoch에서는 train 데이터셋의 loss와 accuracy는 validation 데이터셋의 loss와 accuracy에 비해 상대적으로 큰 차이가 있습니다.

그 다음 epoch에서는 train 데이터셋의 loss와 accuracy가 점차적으로 개선되어가며, validation 데이터셋의 loss와 accuracy도 개선되어가는 것을 볼 수 있습니다. 특히, 5번째 epoch에서 validation 데이터셋의 accuracy가 0.9922로 가장 높게 나오는 것을 확인할 수 있습니다.

최종 epoch에서는 train 데이터셋의 loss는 0.0100, accuracy는 0.9973이며, validation 데이터셋의 loss는 0.0498, accuracy는 0.9903입니다. 즉, train 데이터셋에서는 매우 높은 성능을 보이지만, validation 데이터셋에서는 약간의 overfitting이 발생한 것으로 보입니다.
